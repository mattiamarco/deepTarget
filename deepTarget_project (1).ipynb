{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMa9tKjTZ_fK"
   },
   "source": [
    "# Deep learning model to identify target mRNA of microRNA sequences\n",
    "\n",
    "From a review of currently developed Deep learning models in genomics, I realized there is still a long way to go before the full stock of machine learning techniques will be applied to genomics at its maximum potential. In particular, I got fascinated by the deepTarget model that has recently been proposed as a way to identify microRNA targets with 96% accuracy - therefore I decided to focus my project in this field. The repository of the mentioned work is available on GitHib at the following link: https://github.com/ailab-seoultech/deepTarget.\n",
    "\n",
    "MicroRNAs (miRNAs), which are small non-coding RNA molecules that consist of about 22 nucleotides, are known to regulate more than 60% of protein coding genes of humans and other mammals at the RNA level. As miRNAs control the function of their target messenger RNAs (mRNAs) by regulating the expression of the targets, investigating miRNAs is important to understand various biological processes, including diseases. To predict targets of given miRNAs, numerous computational tools have been proposed. \n",
    "\n",
    "## Problem Statement\n",
    "Two types of computational problems about miRNAs thus naturally arise in bioinformatics: miRNA host identification (i.e., the problem of locating the genes that encode pre-miRNAs) and miRNA target prediction (i.e., the task of finding the mRNA targets of given miRNAs). This project focuses on the target prediction problem, aiming at deploying a model that is able to recognize whether or not a sequence of microRNA belongs to a given target mRNA. The output will be a binary classification of miRNA-mRNA pairs that match/ don’t match (i.e. mRNA is a target of the given miRNA).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUjS-KPpSTFp"
   },
   "source": [
    "## Getting ready\n",
    "Copyin the dataset from GitHub repository of the author of the paper and importing the right libraries\n",
    "Le'ts first install the right version of the softwares to use and import the needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-9dfnOx818P",
    "outputId": "ac08a401-93e6-4edf-ba84-7de7d9a0a4b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'deepTarget' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/ailab-seoultech/deepTarget.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-u4hA1U89LA",
    "outputId": "2972753b-bc6e-4937-b32c-c14afc15404e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data.tar.gz...\n",
      "Extracting data.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "! sh deepTarget/download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2nY4cRANuZ7",
    "outputId": "bbda5459-28fa-4c2f-f510-ed0a31b4ea77",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.78-cp36-cp36m-manylinux1_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 17.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from biopython) (1.19.5)\n",
      "Installing collected packages: biopython\n",
      "Successfully installed biopython-1.78\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 776.8 MB 5.3 kB/s  eta 0:00:01     |████▊                           | 116.0 MB 76.5 MB/s eta 0:00:09     |███████▉                        | 189.7 MB 82.5 MB/s eta 0:00:08MB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: dataclasses, torch\n",
      "Successfully installed dataclasses-0.8 torch-1.7.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: regex\n",
      "Successfully installed regex-2020.11.13\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iw-H4SwJkFyU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "bar_format = '{desc} |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}{postfix}]'\n",
    "import sklearn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "\n",
    "import argparse\n",
    "import regex\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement nomkl\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for nomkl\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nomkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkSXa_wdaXAw"
   },
   "source": [
    "# Pre-processing\n",
    "I will now define a set of functions that will help me convert a csv formatted dataset into an input to feed the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4UaUPYcjgxz"
   },
   "source": [
    "##Dataset\n",
    "The dataset includes: \n",
    "* 2 '.fasta' files that encode the sequence of mRNA and miRNA\n",
    "* a training set with pairs of miRNA and mRNA sequences\n",
    "* a test set with only miRNA and mRNA endoded versions\n",
    "\n",
    "Let's first define a range of basic functions to read the '.fasta' files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyArlMNfkMr9",
    "outputId": "7076e8b1-90a7-4dcd-f66f-7a8a64d5e93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mirna_ID : hsa-miR-4777-5p. , mirna_SEQUENCE : UUCUAGAUGAGAGAUAUAUAUA\n",
      "mirna_ID : hsa-miR-3908. , mirna_SEQUENCE : GAGCAAUGUAGGUAGACUGUUU\n",
      "mirna_ID : hsa-miR-96-3p. , mirna_SEQUENCE : AAUCAUGUGCAGUGCCAAUAUG\n",
      "mirna_ID : hsa-miR-3144-5p. , mirna_SEQUENCE : AGGGGACCAAAGAGAUAUAUAG\n",
      "mirna_ID : hsa-miR-6509-3p. , mirna_SEQUENCE : UUCCACUGCCACUACCUAAUUU\n",
      "\n",
      "\n",
      "mrna_ID : NM_003629. , mrna_SEQUENCE : AGAGGAAGUG\n",
      "mrna_ID : NM_001135041. , mrna_SEQUENCE : GCACUCCUUU\n",
      "mrna_ID : NM_001256461. , mrna_SEQUENCE : AUGUUCUAUA\n",
      "mrna_ID : NM_005371. , mrna_SEQUENCE : CUGCUUACUC\n",
      "mrna_ID : NM_175734. , mrna_SEQUENCE : CCAGCAGGCG\n"
     ]
    }
   ],
   "source": [
    "def read_fasta(mirna_fasta_file, mrna_fasta_file):\n",
    "    # function to read the '.fatsa' files and retun sequences and ids of \n",
    "    # mRNA and miRNA\n",
    "    mirna_list = list(SeqIO.parse(mirna_fasta_file, 'fasta'))\n",
    "    mrna_list = list(SeqIO.parse(mrna_fasta_file, 'fasta'))\n",
    "    \n",
    "    mirna_ids = []\n",
    "    mirna_seqs = []\n",
    "    mrna_ids = []\n",
    "    mrna_seqs = []\n",
    "    \n",
    "    for i in range(len(mirna_list)):\n",
    "        mirna_ids.append(str(mirna_list[i].id))\n",
    "        mirna_seqs.append(str(mirna_list[i].seq))\n",
    "    \n",
    "    for i in range(len(mrna_list)):\n",
    "        mrna_ids.append(str(mrna_list[i].id))\n",
    "        mrna_seqs.append(str(mrna_list[i].seq))\n",
    "    \n",
    "    return mirna_ids, mirna_seqs, mrna_ids, mrna_seqs\n",
    "\n",
    "\n",
    "mirna_fasta_file = \"data/mirna.fasta\"\n",
    "mrna_fasta_file  = \"data/mrna.fasta\"\n",
    "mirna_ids, mirna_seqs, mrna_ids, mrna_seqs = read_fasta(mirna_fasta_file,mrna_fasta_file)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"mirna_ID : {mirna_ids[i]}. , mirna_SEQUENCE : {mirna_seqs[i]}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"mrna_ID : {mrna_ids[i]}. , mrna_SEQUENCE : {mrna_seqs[i][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5ms9t5toV4h",
    "outputId": "f6a929ae-8081-4a4d-cf65-4bd3523f4d87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 1 2 4 2 2 4 4 4 2 2 2 2 4 3 2 4 3 4 2 2 2 2 4 4 2 3 1 2 2 2 4 2 1 3 2\n",
      " 2 2 4 2 4 3 3 4 3 2 2 3 2 4 2 4 3 2 2 2 3 1 4 3 2 1 2 1 3 2 2 1 2 2 4 2 1\n",
      " 3 2 2 1 3 2 2 2 2 2 1 3 3 4 1 3 1 1 1 2 3 4 3 3 3 4 4 1 1 3 2 4 2 4 4 2 2\n",
      " 4 3 2 2 2 2 3 4 4 2 1 3 2 4 4 2 1 2 4 2 2 2 1 2 2 2 4 4 4 2 1 3 2 3 4 2 2\n",
      " 4 3 2 2 2 2 4 4 2 1 2 2 4 4 3 1 2 2 2 3 3 3 4 4 2 2 2 2 2 1 2 4 2 2 2 1 4\n",
      " 4 2 2 2 4 3 3 2 2 4 2 4 3 2 2 1 4 1 1 4 4 4 3 4 4 3 4 4 2 1 1 2 4 3 2 4 2\n",
      " 2 2 4 2 2 4 4 2 2 4 3 1 3 3 3 3 2 2 4 2 1 3 3 3 2 4 4 3 4 3 3 3 3 3 3 4 1\n",
      " 3 3 2 4 3 1 3 1 2 2 2 2 1 2 2 1 2 2 1 1 1 3 3 4 4 1 1 3 4 3 1 3 3 4 2 2 2\n",
      " 2 4 4 3 1 4 4 3 1 3 3 1 2 4 4 2 1 2 2 2 2 4 4 3 1 4 4 1 1 1 3 2 1 1 2 4 4\n",
      " 2 4 3 2 4 4 2 1 3 4 3 2]\n"
     ]
    }
   ],
   "source": [
    "def nucleotide_to_int(nucleotides, max_len):\n",
    "    # assign a number to the basis in order to translate nucleotides into int\n",
    "    dictionary = {'A':1, 'C':2, 'G':3, 'T':4, 'U':4}\n",
    "    \n",
    "    chars = []\n",
    "    nucleotides = nucleotides.upper()\n",
    "    for c in nucleotides:\n",
    "        chars.append(c)\n",
    "    \n",
    "    ints_enc = np.full((max_len,), fill_value=0) # to post-pad inputs\n",
    "    for i in range(len(chars)):\n",
    "        try:\n",
    "            ints_enc[i] = dictionary[chars[i]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except IndexError:\n",
    "            break\n",
    "        \n",
    "    return ints_enc\n",
    "\n",
    "\n",
    "seq = mrna_seqs[1]\n",
    "max_len = len(seq)\n",
    "\n",
    "ints_enc = nucleotide_to_int(seq,max_len)\n",
    "\n",
    "print(ints_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0SxQqahy8U_",
    "outputId": "ffdaf2a7-8c10-4e47-a1a2-d3f08ed5b72a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 1 2 4 2 2 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "def sequence_to_int(sequences, max_len):\n",
    "    # translate entire RNA sequences into int\n",
    "    import itertools\n",
    "    \n",
    "    if type(sequences) is list:\n",
    "        seqs_enc = np.asarray([nucleotide_to_int(seq, max_len) for seq in sequences])\n",
    "    else:\n",
    "        seqs_enc = np.asarray([nucleotide_to_int(seq, max_len) for seq in sequences])\n",
    "        seqs_enc = list(itertools.chain(*seqs_enc))\n",
    "        seqs_enc = np.asarray(seqs_enc)\n",
    "    \n",
    "    return seqs_enc\n",
    "\n",
    "x=sequence_to_int(mrna_seqs, 10)\n",
    "print (x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1vLw5DDzrat",
    "outputId": "29da04e4-d211-431f-9e8d-c31838a78009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 1 2 4 2 2 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences(sequences, max_len=None, padding='pre', fill_value=0):\n",
    "    # padding sequences with fill_value to achieve seme lenght\n",
    "    n_samples = len(sequences)\n",
    "    \n",
    "    lengths = []\n",
    "    for seq in sequences:\n",
    "        try:\n",
    "            lengths.append(len(seq))\n",
    "        except TypeError:\n",
    "            raise ValueError(\"sequences expected a list of iterables, got {}\".format(seq))\n",
    "    if max_len is None:\n",
    "        max_len = np.max(lengths)\n",
    "    \n",
    "    input_shape = np.asarray(sequences[0]).shape[1:]\n",
    "    padded_shape = (n_samples, max_len) + input_shape\n",
    "    padded = np.full(padded_shape, fill_value=fill_value)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        if padding == 'pre':\n",
    "            truncated = seq[-max_len:]\n",
    "            padded[i, -len(truncated):] = truncated\n",
    "        elif padding == 'post':\n",
    "            truncated = seq[:max_len]\n",
    "            padded[i, :len(truncated)] = truncated\n",
    "        else:\n",
    "            raise ValueError(\"padding expected 'pre' or 'post', got {}\".format(truncating))\n",
    "    \n",
    "    return padded\n",
    "\n",
    "x=pad_sequences(x, max_len=100)\n",
    "print (x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K68XiRph0hLd",
    "outputId": "d1f8f6b9-278f-474a-bad7-161d0c2b7c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot(ints):\n",
    "    #one hot encoding for nucleotides\n",
    "    dictionary_k = 5 # maximum number of nucleotides\n",
    "    ints_len = len(ints)\n",
    "    ints_enc = np.zeros((ints_len, dictionary_k))\n",
    "    ints_enc[np.arange(ints_len), [k for k in ints]] = 1\n",
    "    ints_enc = ints_enc[:, 1:5] # to handle zero-padded values\n",
    "    ints_enc = ints_enc.tolist()\n",
    "    \n",
    "    return (ints_enc)\n",
    "\n",
    "def one_hot_enc(seqs_enc):\n",
    "    #one hot encoding for sequences    \n",
    "    one_hot_encs = []\n",
    "    \n",
    "    for i in range(len(seqs_enc)):\n",
    "        one_hot_encs.append(one_hot(seqs_enc[i]))\n",
    "    \n",
    "    one_hot_encs = np.array(one_hot_encs)\n",
    "    \n",
    "    return one_hot_encs\n",
    "\n",
    "b=one_hot(x[1])\n",
    "print (b[95:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRkECXQ733ld",
    "outputId": "9657aa23-5f3a-4fb2-9ce8-b64d8daf3bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def to_categorical(labels, n_classes=None):\n",
    "    #matrix assigning labels to a number of classes\n",
    "    labels = np.array(labels, dtype='int').reshape(-1)\n",
    "\n",
    "    n_samples = labels.shape[0]\n",
    "    if not n_classes:\n",
    "        n_classes = np.max(labels) + 1\n",
    "\n",
    "    categorical = np.zeros((n_samples, n_classes))\n",
    "    categorical[np.arange(n_samples), labels] = 1\n",
    "    \n",
    "    return categorical\n",
    "\n",
    "labels=[0,1,1,1,1,0,2,3,4,0,1,2,3]\n",
    "labels=to_categorical(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hdNFVjVinHjf"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(x_query_seqs, x_target_seqs, y=None, cts_size=None, pre_padding=False):\n",
    "    #getting encoded data form miRNA and mRNA sequences\n",
    "    if cts_size is not None:\n",
    "        max_len = cts_size\n",
    "    else:\n",
    "        max_len = max(len(max(x_query_seqs, key=len)), len(max(x_target_seqs, key=len)))\n",
    "    \n",
    "    x_mirna = sequence_to_int(x_query_seqs, max_len)\n",
    "    x_mrna = sequence_to_int(x_target_seqs, max_len)\n",
    "    \n",
    "    if pre_padding:\n",
    "        x_mirna = pad_sequences(x_mirna, max_len, padding='pre')\n",
    "        x_mrna = pad_sequences(x_mrna, max_len, padding='pre')\n",
    "    \n",
    "    x_mirna_embd = one_hot_enc(x_mirna)\n",
    "    x_mrna_embd = one_hot_enc(x_mrna)\n",
    "    if y is not None:\n",
    "        y_embd = to_categorical(y, np.unique(y).size)\n",
    "        \n",
    "        return x_mirna_embd, x_mrna_embd, y_embd\n",
    "    else:\n",
    "        return x_mirna_embd, x_mrna_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(2+2)\n",
    "x_mirna_embd, x_mrna_embd=preprocess_data(mirna_seqs, mrna_seqs)\n",
    "print(2+2)\n",
    "\n",
    "print (x_mirna_embd, x_mrna_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJ7F04It3qZn"
   },
   "source": [
    "Let's now define how matching can happen\n",
    "Following the paper, I utilize relaxed site patterns, which covers most of the canonical site types (CSTs), non-canonical site types (NSTs), and context-dependent non-canonical site types (CDNSTs, to define the candidate target site (CTS). The details used are as follows:\n",
    "* 10-mer-m6: six WC pairings from the miRNA nucleotides 1–10\n",
    "* 10-mer-m7: seven WC pairings from the miRNA nucleotides 1–10\n",
    "* Offset 9-mer-m7: seven WC pairings from the miRNA nucleotides 2–10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "99eN8pN65qad"
   },
   "outputs": [],
   "source": [
    "def find_candidate(mirna_sequence, mrna_sequence, seed_match):\n",
    "    #find potential matched with tolerance\n",
    "    positions = set()\n",
    "    \n",
    "    if seed_match == '10-mer-m6':\n",
    "        SEED_START = 1\n",
    "        SEED_END = 10\n",
    "        SEED_OFFSET = SEED_START - 1\n",
    "        MIN_MATCH = 6\n",
    "        TOLERANCE = (SEED_END-SEED_START+1) - MIN_MATCH\n",
    "    elif seed_match == '10-mer-m7':\n",
    "        SEED_START = 1\n",
    "        SEED_END = 10\n",
    "        SEED_OFFSET = SEED_START - 1\n",
    "        MIN_MATCH = 7\n",
    "        TOLERANCE = (SEED_END-SEED_START+1) - MIN_MATCH\n",
    "    elif seed_match == 'offset-9-mer-m7':\n",
    "        SEED_START = 2\n",
    "        SEED_END = 10\n",
    "        SEED_OFFSET = SEED_START - 1\n",
    "        MIN_MATCH = 7\n",
    "        TOLERANCE = (SEED_END-SEED_START+1) - MIN_MATCH\n",
    "    elif seed_match == 'strict':\n",
    "        positions = find_strict_candidate(mirna_sequence, mrna_sequence)\n",
    "        \n",
    "        return positions\n",
    "    else:\n",
    "        raise ValueError(\"seed_match expected 'strict', '10-mer-m6', '10-mer-m7', or 'offset-9-mer-m7', got '{}'\".format(seed_match))\n",
    "    \n",
    "    seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "    rc_seed = str(Seq(seed).complement())\n",
    "    match_iter = regex.finditer(\"({}){{e<={}}}\".format(rc_seed, TOLERANCE), mrna_sequence)\n",
    "    \n",
    "    for match_index in match_iter:\n",
    "        #positions.add(match_index.start()) # slice-start indicies\n",
    "        positions.add(match_index.end()+SEED_OFFSET) # slice-stop indicies\n",
    "    \n",
    "    positions = list(positions)\n",
    "    \n",
    "    return positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qReji6mOehUD"
   },
   "outputs": [],
   "source": [
    "positions=find_candidate(mirna_seqs[1], mrna_seqs[1], seed_match = '10-mer-m6')\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9MHQcIDa5EB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_strict_candidate(mirna_sequence, mrna_sequence):\n",
    "    #find potential matched without tolerance\n",
    "\n",
    "    positions = set()\n",
    "    \n",
    "    SEED_TYPES = ['8-mer', '7-mer-m8', '7-mer-A1', '6-mer', '6-mer-A1', 'offset-7-mer', 'offset-6-mer']\n",
    "    for seed_match in SEED_TYPES:\n",
    "        if seed_match == '8-mer':\n",
    "            SEED_START = 2\n",
    "            SEED_END = 8\n",
    "            SEED_OFFSET = 0\n",
    "            seed = 'U' + mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == '7-mer-m8':\n",
    "            SEED_START = 1\n",
    "            SEED_END = 8\n",
    "            SEED_OFFSET = 0\n",
    "            seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == '7-mer-A1':\n",
    "            SEED_START = 2\n",
    "            SEED_END = 7\n",
    "            SEED_OFFSET = 0\n",
    "            seed = 'U' + mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == '6-mer':\n",
    "            SEED_START = 2\n",
    "            SEED_END = 7\n",
    "            SEED_OFFSET = 1\n",
    "            seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == '6mer-A1':\n",
    "            SEED_START = 2\n",
    "            SEED_END = 6\n",
    "            SEED_OFFSET = 0\n",
    "            seed = 'U' + mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == 'offset-7-mer':\n",
    "            SEED_START = 3\n",
    "            SEED_END = 9\n",
    "            SEED_OFFSET = 0\n",
    "            seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == 'offset-6-mer':\n",
    "            SEED_START = 3\n",
    "            SEED_END = 8\n",
    "            SEED_OFFSET = 0\n",
    "            seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        \n",
    "        rc_seed = str(Seq(seed).complement())\n",
    "        match_iter = regex.finditer(rc_seed, mrna_sequence)\n",
    "        \n",
    "        for match_index in match_iter:\n",
    "            #positions.add(match_index.start()) # slice-start indicies\n",
    "            positions.add(match_index.end()+SEED_OFFSET) # slice-stop indicies\n",
    "    \n",
    "    positions = list(positions)\n",
    "    \n",
    "    return positions\n",
    "\n",
    "\n",
    "def get_candidate(mirna_sequence, mrna_sequence, cts_size, seed_match):\n",
    "    #using the find_candidate function we can find actual candidates and positions\n",
    "    positions = find_candidate(mirna_sequence, mrna_sequence, seed_match)\n",
    "    \n",
    "    candidates = []\n",
    "    for i in positions:\n",
    "        site_sequence = mrna_sequence[max(0, i-cts_size):i]\n",
    "        rev_site_sequence = site_sequence[::-1]\n",
    "        rc_site_sequence = str(Seq(rev_site_sequence).complement())\n",
    "        candidates.append(rev_site_sequence) # miRNAs: 5'-ends to 3'-ends,  mRNAs: 3'-ends to 5'-ends\n",
    "        #candidates.append(rc_site_sequence)\n",
    "    \n",
    "    return candidates, positions\n",
    "\n",
    "\n",
    "def make_pair(mirna_sequence, mrna_sequence, cts_size, seed_match):\n",
    "    #and finally identify mirna_querys and mrna_targets\n",
    "    candidates, positions = get_candidate(mirna_sequence, mrna_sequence, cts_size, seed_match)\n",
    "    \n",
    "    mirna_querys = []\n",
    "    mrna_targets = []\n",
    "    if len(candidates) == 0:\n",
    "        return (mirna_querys, mrna_targets, positions)\n",
    "    else:\n",
    "        mirna_sequence = mirna_sequence[0:cts_size]\n",
    "        for i in range(len(candidates)):\n",
    "            mirna_querys.append(mirna_sequence)\n",
    "            mrna_targets.append(candidates[i])\n",
    "        \n",
    "    return mirna_querys, mrna_targets, positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UENUtYs30IxG"
   },
   "source": [
    "We also need some functions to read the training and test files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lj5f655V0MaN"
   },
   "outputs": [],
   "source": [
    "def read_ground_truth(ground_truth_file, header=True, train=True):\n",
    "    # read the trainign and test files containing pairs of miRNA-mRNA \n",
    "    # input format: [miRNA_ID, mRNA_ID, LABEL]\n",
    "    if header is True:\n",
    "        records = pd.read_csv(ground_truth_file, header=0, sep='\\t')\n",
    "    else:\n",
    "        records = pd.read_csv(ground_truth_file, header=None, sep='\\t')\n",
    "    \n",
    "    query_ids = np.asarray(records.iloc[:, 0].values)\n",
    "    target_ids = np.asarray(records.iloc[:, 1].values)\n",
    "    if train is True:\n",
    "        labels = np.asarray(records.iloc[:, 2].values)\n",
    "    else:\n",
    "        labels = np.full((len(records),), fill_value=-1)\n",
    "    \n",
    "    return query_ids, target_ids, labels\n",
    "\n",
    "\n",
    "def make_input_pair(mirna_fasta_file, mrna_fasta_file, ground_truth_file, cts_size=30, seed_match='offset-9-mer-m7', header=True, train=True):\n",
    "    #from sequences, ids and ground truth we generate the dataset\n",
    "    mirna_ids, mirna_seqs, mrna_ids, mrna_seqs = read_fasta(mirna_fasta_file, mrna_fasta_file)\n",
    "    query_ids, target_ids, labels = read_ground_truth(ground_truth_file, header=header, train=train)\n",
    "    \n",
    "    dataset = {\n",
    "        'mirna_fasta_file': mirna_fasta_file,\n",
    "        'mrna_fasta_file': mrna_fasta_file,\n",
    "        'ground_truth_file': ground_truth_file,\n",
    "        'query_ids': [],\n",
    "        'query_seqs': [],\n",
    "        'target_ids': [],\n",
    "        'target_seqs': [],\n",
    "        'target_locs': [],\n",
    "        'labels': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(query_ids)):\n",
    "        try:\n",
    "            j = mirna_ids.index(query_ids[i])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        try:\n",
    "            k = mrna_ids.index(target_ids[i])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        query_seqs, target_seqs, locations = make_pair(mirna_seqs[j], mrna_seqs[k], cts_size=cts_size, seed_match=seed_match)\n",
    "        \n",
    "        n_pairs = len(locations)\n",
    "        if n_pairs > 0:\n",
    "            queries = [query_ids[i] for n in range(n_pairs)]\n",
    "            dataset['query_ids'].extend(queries)\n",
    "            dataset['query_seqs'].extend(query_seqs)\n",
    "            \n",
    "            targets = [target_ids[i] for n in range(n_pairs)]\n",
    "            dataset['target_ids'].extend(targets)\n",
    "            dataset['target_seqs'].extend(target_seqs)\n",
    "            dataset['target_locs'].extend(locations)\n",
    "            \n",
    "            dataset['labels'].extend([[labels[i]] for p in range(n_pairs)])\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def make_brute_force_pair(mirna_fasta_file, mrna_fasta_file, cts_size=30, seed_match='offset-9-mer-m7'):\n",
    "    mirna_ids, mirna_seqs, mrna_ids, mrna_seqs = read_fasta(mirna_fasta_file, mrna_fasta_file)\n",
    "    \n",
    "    dataset = {\n",
    "        'query_ids': [],\n",
    "        'query_seqs': [],\n",
    "        'target_ids': [],\n",
    "        'target_seqs': [],\n",
    "        'target_locs': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(mirna_ids)):\n",
    "        for j in range(len(mrna_ids)):\n",
    "            query_seqs, target_seqs, positions = make_pair(mirna_seqs[i], mrna_seqs[j], cts_size, seed_match)\n",
    "            \n",
    "            n_pairs = len(positions)\n",
    "            if n_pairs > 0:\n",
    "                query_ids = [mirna_ids[i] for k in range(n_pairs)]\n",
    "                dataset['query_ids'].extend(query_ids)\n",
    "                dataset['query_seqs'].extend(query_seqs)\n",
    "                \n",
    "                target_ids = [mrna_ids[j] for k in range(n_pairs)]\n",
    "                dataset['target_ids'].extend(target_ids)\n",
    "                dataset['target_seqs'].extend(target_seqs)\n",
    "                dataset['target_locs'].extend(positions)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEnv0A1jxz9Z"
   },
   "source": [
    "Let's now prepare the dataset using the above basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_wDFGrOx0Zi"
   },
   "outputs": [],
   "source": [
    "def get_negative_pair(mirna_fasta_file, mrna_fasta_file, ground_truth_file=None, cts_size=30, seed_match='offset-9-mer-m7', header=False, predict_mode=True):\n",
    "    #prepare dataset with query, target and predictions\n",
    "    mirna_ids, mirna_seqs, mrna_ids, mrna_seqs = read_fasta(mirna_fasta_file, mrna_fasta_file)\n",
    "    \n",
    "    dataset = {\n",
    "        'query_ids': [],\n",
    "        'target_ids': [],\n",
    "        'predicts': []\n",
    "    }\n",
    "    \n",
    "    if ground_truth_file is not None:\n",
    "        query_ids, target_ids, labels = read_ground_truth(ground_truth_file, header=header)\n",
    "        \n",
    "        for i in range(len(query_ids)):\n",
    "            try:\n",
    "                j = mirna_ids.index(query_ids[i])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            try:\n",
    "                k = mrna_ids.index(target_ids[i])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            query_seqs, target_seqs, locations = make_pair(mirna_seqs[j], mrna_seqs[k], cts_size=cts_size, seed_match=seed_match)\n",
    "\n",
    "            n_pairs = len(locations)\n",
    "            if (n_pairs == 0) and (predict_mode is True):\n",
    "                dataset['query_ids'].append(query_ids[i])\n",
    "                dataset['target_ids'].append(target_ids[i])\n",
    "                dataset['predicts'].append(0)\n",
    "            elif (n_pairs == 0) and (predict_mode is False):\n",
    "                dataset['query_ids'].append(query_ids[i])\n",
    "                dataset['target_ids'].append(target_ids[i])\n",
    "                dataset['predicts'].append(labels[i])\n",
    "    else:\n",
    "        for i in range(len(mirna_ids)):\n",
    "            for j in range(len(mrna_ids)):\n",
    "                query_seqs, target_seqs, locations = make_pair(mirna_seqs[i], mrna_seqs[j], cts_size=cts_size, seed_match=seed_match)\n",
    "\n",
    "                n_pairs = len(locations)\n",
    "                if n_pairs == 0:\n",
    "                    dataset['query_ids'].append(mirna_ids[i])                \n",
    "                    dataset['target_ids'].append(mrna_ids[j])\n",
    "                    dataset['predicts'].append(0)\n",
    "    \n",
    "    dataset['target_locs'] = [-1 for i in range(len(dataset['query_ids']))]\n",
    "    dataset['probabilities'] = [0.0 for i in range(len(dataset['query_ids']))]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def postprocess_result(dataset, probabilities, predicts, predict_mode=True, output_file=None, cts_size=30, seed_match='offset-9-mer-m7', level='site'):\n",
    "    neg_pairs = get_negative_pair(dataset['mirna_fasta_file'], dataset['mrna_fasta_file'], dataset['ground_truth_file'], cts_size=cts_size, seed_match=seed_match, predict_mode=predict_mode)\n",
    "    \n",
    "    query_ids = np.append(dataset['query_ids'], neg_pairs['query_ids'])\n",
    "    target_ids = np.append(dataset['target_ids'], neg_pairs['target_ids'])\n",
    "    target_locs = np.append(dataset['target_locs'], neg_pairs['target_locs'])\n",
    "    probabilities = np.append(probabilities, neg_pairs['probabilities'])\n",
    "    predicts = np.append(predicts, neg_pairs['predicts'])\n",
    "    \n",
    "    # output format: [QUERY, TARGET, LOCATION, PROBABILITY]\n",
    "    records = pd.DataFrame(columns=['MIRNA_ID', 'MRNA_ID', 'LOCATION', 'PROBABILITY'])\n",
    "    records['MIRNA_ID'] = query_ids\n",
    "    records['MRNA_ID'] = target_ids\n",
    "    records['LOCATION'] = np.array([\"{},{}\".format(max(1, l-cts_size+1), l) if l != -1 else \"-1,-1\" for l in target_locs])\n",
    "    records['PROBABILITY'] = probabilities\n",
    "    if predict_mode is True:\n",
    "        records['PREDICT'] = predicts\n",
    "    else:\n",
    "        records['LABEL'] = predicts\n",
    "    \n",
    "    records = records.sort_values(by=['PROBABILITY', 'MIRNA_ID', 'MRNA_ID'], ascending=[False, True, True])\n",
    "    unique_records = records.sort_values(by=['PROBABILITY', 'MIRNA_ID', 'MRNA_ID'], ascending=[False, True, True]).drop_duplicates(subset=['MIRNA_ID', 'MRNA_ID'], keep='first')\n",
    "    \n",
    "    if level == 'site':\n",
    "        if output_file is not None:\n",
    "            records.to_csv(output_file, index=False, sep='\\t')\n",
    "        \n",
    "        return records\n",
    "    elif level == 'gene':\n",
    "        if output_file is not None:\n",
    "            unique_records.to_csv(output_file, index=False, sep='\\t')\n",
    "        \n",
    "        return unique_records\n",
    "    else:\n",
    "        raise ValueError(\"level expected 'site' or 'gene', got '{}'\".format(mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyvNiVOZyvET"
   },
   "source": [
    "Time to setup the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQmlodwsywZH"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    #herediting the torch class for the dataset???\n",
    "    def __init__(self, mirna_fasta_file, mrna_fasta_file, ground_truth_file, cts_size=30, seed_match='offset-9-mer-m7', header=True, train=True):\n",
    "        #setup with fuctions above\n",
    "        self.dataset = make_input_pair(mirna_fasta_file, mrna_fasta_file, ground_truth_file, cts_size=cts_size, seed_match=seed_match, header=header, train=train)\n",
    "        self.mirna, self.mrna = preprocess_data(self.dataset['query_seqs'], self.dataset['target_seqs'])\n",
    "        self.labels = np.asarray(self.dataset['labels']).reshape(-1,)\n",
    "        \n",
    "        self.mirna = self.mirna.transpose((0, 2, 1))\n",
    "        self.mrna = self.mrna.transpose((0, 2, 1))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "            \n",
    "        mirna = self.mirna[index]\n",
    "        mrna = self.mrna[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        return (mirna, mrna), label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    #herediting the torch class for the training dataset???\n",
    "    def __init__(self, ground_truth_file, cts_size=30):\n",
    "        #initialize with train set (ground_truth)\n",
    "        self.records = pd.read_csv(ground_truth_file, header=0, sep='\\t')\n",
    "        mirna_seqs = self.records['MIRNA_SEQ'].values.tolist()\n",
    "        mrna_seqs = self.records['MRNA_SEQ'].values.tolist()\n",
    "        self.mirna, self.mrna = preprocess_data(mirna_seqs, mrna_seqs, cts_size=cts_size)\n",
    "        self.labels = self.records['LABEL'].values.astype(int)\n",
    "        \n",
    "        self.mirna = self.mirna.transpose((0, 2, 1))\n",
    "        self.mrna = self.mrna.transpose((0, 2, 1))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        \n",
    "        mirna = self.mirna[index]\n",
    "        mrna = self.mrna[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        return (mirna, mrna), label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ESX-Tln01VH"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXgFZyFd0nZJ"
   },
   "outputs": [],
   "source": [
    "class HyperParam:\n",
    "    def __init__(self, filters=None, kernels=None, model_json=None):\n",
    "        self.dictionary = dict()\n",
    "        self.name_postfix = str()\n",
    "\n",
    "        if (filters is not None) and (kernels is not None) and (model_json is None):\n",
    "            for i, (f, k) in enumerate(zip(filters, kernels)):\n",
    "                setattr(self, 'f{}'.format(i+1), f)\n",
    "                setattr(self, 'k{}'.format(i+1), k)\n",
    "                self.dictionary.update({'f{}'.format(i+1): f, 'k{}'.format(i+1): k})\n",
    "            self.len = i+1\n",
    "                \n",
    "            for key, value in self.dictionary.items():\n",
    "                self.name_postfix = \"{}_{}-{}\".format(self.name_postfix, key, value)\n",
    "        elif model_json is not None:\n",
    "            self.dictionary = json.loads(model_json)\n",
    "            for i, (key, value) in enumerate(self.dictionary.items()):\n",
    "                setattr(self, key, value)\n",
    "                self.name_postfix = \"{}_{}-{}\".format(self.name_postfix, key, value)\n",
    "            self.len = (i+1)//2\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class deepTarget(nn.Module):\n",
    "    def __init__(self, hparams=None, hidden_units=30, input_shape=(1, 4, 30), name_prefix=\"model\"):\n",
    "        super(deepTarget, self).__init__()\n",
    "        \n",
    "        if hparams is None:\n",
    "            filters, kernels = [32, 16, 64, 16], [3, 3, 3, 3]\n",
    "            hparams = HyperParam(filters, kernels)\n",
    "        self.name = \"{}{}\".format(name_prefix, hparams.name_postfix)\n",
    "        \n",
    "        if (isinstance(hparams, HyperParam)) and (len(hparams) == 4):\n",
    "            self.embd1 = nn.Conv1d(4, hparams.f1, kernel_size=hparams.k1, padding=((hparams.k1 - 1) // 2))\n",
    "            self.conv2 = nn.Conv1d(hparams.f1*2, hparams.f2, kernel_size=hparams.k2)\n",
    "            self.conv3 = nn.Conv1d(hparams.f2, hparams.f3, kernel_size=hparams.k3)\n",
    "            self.conv4 = nn.Conv1d(hparams.f3, hparams.f4, kernel_size=hparams.k4)\n",
    "            \n",
    "            \"\"\" out_features = ((in_length - kernel_size + (2 * padding)) / stride + 1) * out_channels \"\"\"\n",
    "            flat_features = self.forward(torch.rand(input_shape), torch.rand(input_shape), flat_check=True)\n",
    "            self.fc1 = nn.Linear(flat_features, hidden_units)\n",
    "            self.fc2 = nn.Linear(hidden_units, 2)\n",
    "        else:\n",
    "            raise ValueError(\"not enough hyperparameters\")\n",
    "    \n",
    "    def forward(self, x_mirna, x_mrna, flat_check=False):\n",
    "        h_mirna = F.relu(self.embd1(x_mirna))\n",
    "        h_mrna = F.relu(self.embd1(x_mrna))\n",
    "        \n",
    "        h = torch.cat((h_mirna, h_mrna), dim=1)\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.relu(self.conv4(h))\n",
    "        \n",
    "        h = h.view(h.size(0), -1)\n",
    "        if flat_check:\n",
    "            return h.size(1)\n",
    "        h = self.fc1(h)\n",
    "        y = self.fc2(h) #y = F.softmax(self.fc2(h), dim=1)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def size(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoqQg2fA0j3B"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S82-L9LuyyTT"
   },
   "outputs": [],
   "source": [
    "def train_model(mirna_fasta_file, mrna_fasta_file, train_file, model=None, cts_size=30, seed_match='offset-9-mer-m7', level='gene', batch_size=32, epochs=10, save_file=None, device='cpu'):\n",
    "    if not isinstance(model, deepTarget):\n",
    "        raise ValueError(\"'model' expected <nn.Module 'deepTarget'>, got {}\".format(type(model)))\n",
    "    \n",
    "    print(\"\\n[TRAIN] {}\".format(model.name))\n",
    "    \n",
    "    if train_file.split('/')[-1] == 'train_set.csv':\n",
    "        train_set = TrainDataset(train_file)\n",
    "    else:\n",
    "        train_set = Dataset(mirna_fasta_file, mrna_fasta_file, train_file, seed_match=seed_match, header=True, train=True)\n",
    "\n",
    "    print(\"BBB\")\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    \n",
    "    class_weight = torch.Tensor(compute_class_weight('balanced', classes=np.unique(train_set.labels), y=train_set.labels)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, corrects = 0, 0\n",
    "\n",
    "        with tqdm(train_loader, desc=\"Epoch {}/{}\".format(epoch+1, epochs), bar_format=bar_format) as tqdm_loader:\n",
    "            for i, ((mirna, mrna), label) in enumerate(tqdm_loader):\n",
    "                print(\"AAA\")\n",
    "                mirna, mrna, label = mirna.to(device, dtype=torch.float), mrna.to(device, dtype=torch.float), label.to(device)\n",
    "                \n",
    "                outputs = model(mirna, mrna)\n",
    "                loss = criterion(outputs, label)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item() * outputs.size(0)\n",
    "                corrects += (torch.max(outputs, 1)[1] == label).sum().item()\n",
    "                \n",
    "                if (i+1) == len(train_loader):\n",
    "                    tqdm_loader.set_postfix(dict(loss=(epoch_loss/len(train_set)), acc=(corrects/len(train_set))))\n",
    "                else:\n",
    "                    tqdm_loader.set_postfix(loss=loss.item())\n",
    "    \n",
    "    if save_file is None:\n",
    "        time = datetime.now()\n",
    "        save_file = \"{}.pt\".format(time.strftime('%Y%m%d_%H%M%S_weights'))\n",
    "    torch.save(model.state_dict(), save_file)\n",
    "    \n",
    "    \n",
    "def predict_result(mirna_fasta_file, mrna_fasta_file, query_file, model=None, weight_file=None, seed_match='offset-9-mer-m7', level='gene', output_file=None, device='cpu'):\n",
    "    if not isinstance(model, deepTarget):\n",
    "        raise ValueError(\"'model' expected <nn.Module 'deepTarget'>, got {}\".format(type(model)))\n",
    "    \n",
    "    if not weight_file.endswith('.pt'):\n",
    "        raise ValueError(\"'weight_file' expected '*.pt', got {}\".format(weight_file))\n",
    "    \n",
    "    model.load_state_dict(torch.load(weight_file))\n",
    "    \n",
    "    test_set = Dataset(mirna_fasta_file, mrna_fasta_file, query_file, seed_match=seed_match, header=True, train=False)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        mirna = torch.from_numpy(test_set.mirna).to(device, dtype=torch.float)\n",
    "        mrna = torch.from_numpy(test_set.mrna).to(device, dtype=torch.float)\n",
    "        label = torch.from_numpy(test_set.labels).to(device)\n",
    "        \n",
    "        outputs = model(mirna, mrna)\n",
    "        _, predicts = torch.max(outputs.data, 1)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        y_probs = probabilities.cpu().numpy()[:, 1]\n",
    "        y_predicts = predicts.cpu().numpy()\n",
    "        y_truth = label.cpu().numpy()\n",
    "        \n",
    "        if output_file is None:\n",
    "            time = datetime.now()\n",
    "            output_file = \"{}.csv\".format(time.strftime('%Y%m%d_%H%M%S_results'))\n",
    "        results = postprocess_result(test_set.dataset, y_probs, y_predicts,\n",
    "                                     seed_match=seed_match, level=level, output_file=output_file)\n",
    "        \n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBCZ4kkvgUKX"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_soplhEgUZd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoZ2W-NO2Nlo"
   },
   "outputs": [],
   "source": [
    "def foo(mode):\n",
    "    mirna_fasta_file = \"data/mirna.fasta\"\n",
    "    mrna_fasta_file  = \"data/mrna.fasta\"\n",
    "    seed_match       = \"offset-9-mer-m7\"\n",
    "    level            = \"gene\"\n",
    "    train_file       = \"data/train/train_set.csv\"\n",
    "    save_file        = \"weights.pt\"\n",
    "    query_file       = \"templates/query_set.csv\"\n",
    "    weight_file      = 'model/weights.pt'\n",
    "    output_file      = \"results.csv\"\n",
    "    batch_size       = 32\n",
    "    epochs           = 10\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if mode == 'train':\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        print(\"\\n[START] {}\".format(start_time.strftime('%Y-%m-%d @ %H:%M:%S')))\n",
    "        \n",
    "        model = deepTarget()\n",
    "        train_model(mirna_fasta_file, mrna_fasta_file, train_file,\n",
    "                    model=model,\n",
    "                    seed_match=seed_match, level=level,\n",
    "                    batch_size=batch_size, epochs=epochs,\n",
    "                    save_file=save_file, device=device)\n",
    "        \n",
    "        finish_time = datetime.now()\n",
    "        print(\"\\n[FINISH] {} (user time: {})\\n\".format(finish_time.now().strftime('%Y-%m-%d @ %H:%M:%S'), (finish_time - start_time)))\n",
    "\n",
    "    elif mode == 'predict':\n",
    "        if query_file is None:\n",
    "            raise ValueError(\"'--query_file' expected '*.csv', got '{}'\".format(configs.QUERY_FILE))\n",
    "            \n",
    "        start_time = datetime.now()\n",
    "        print(\"\\n[START] {}\".format(start_time.strftime('%Y-%m-%d @ %H:%M:%S')))\n",
    "        \n",
    "        model = deepTarget()\n",
    "        results = predict_result(mirna_fasta_file, mrna_fasta_file, query_file,\n",
    "                                 model=model, weight_file=weight_file,\n",
    "                                 seed_match=seed_match, level=level,\n",
    "                                 output_file=output_file, device=device)\n",
    "        \n",
    "        finish_time = datetime.now()\n",
    "        print(\"\\n[FINISH] {} (user time: {})\\n\".format(finish_time.now().strftime('%Y-%m-%d @ %H:%M:%S'), (finish_time - start_time)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    foo(\"train\")\n",
    "    foo(\"predict\")\n",
    "\n",
    "def parse_arguments():\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # parser.add_argument('--mode', dest='MODE', type=str, required=True,\n",
    "    #                     help=\"run mode: [train|predict]\")\n",
    "    \n",
    "    # parser.add_argument('--mirna_file', dest='MIRNA_FASTA_FILE', type=str,\n",
    "    #                     help=\"miRNA fasta file (default: data/miRNA.fasta)\")\n",
    "    # parser.add_argument('--mrna_file', dest='MRNA_FASTA_FILE', type=str,\n",
    "    #                     help=\"mRNA fasta file (default: data/mRNA.fasta)\")\n",
    "    # parser.add_argument('--seed_match', dest='SEED_MATCH', type=str,\n",
    "    #                     help=\"seed match type: [offset-9-mer-m7|10-mer-m7|10-mer-m6] (default: offset-9-mer-m7)\")\n",
    "    # parser.add_argument('--level', dest='LEVEL', type=str,\n",
    "    #                     help=\"prediction level: [gene|site] (default: gene)\")\n",
    "    \n",
    "    # parser.add_argument('--train_file', dest='TRAIN_FILE', type=str,\n",
    "    #                     help=\"training file to be used in 'train' mode (sample: data/train_set.csv)\")\n",
    "    # parser.add_argument('--save_file', dest='SAVE_FILE', type=str,\n",
    "    #                     help=\"state_dict file to be saved in 'train' mode (default: yyyyMMdd_HHmmss_weights.pt)\")\n",
    "    # parser.add_argument('--query_file', dest='QUERY_FILE', type=str,\n",
    "    #                     help=\"query file to be queried in 'predict' mode (sample: templates/query_set.csv)\")\n",
    "    # parser.add_argument('--weight_file', dest='WEIGHT_FILE', type=str,\n",
    "    #                     help=\"state_dict file to be loaded in 'predict' mode (default: model/weights.pt)\")\n",
    "    # parser.add_argument('--output_file', dest='OUTPUT_FILE', type=str,\n",
    "    #                     help=\"output file to be saved in 'predict' mode (default: yyyyMMdd_HHmmss_results.csv)\")\n",
    "    \n",
    "    # parser.add_argument('--batch_size', dest='BATCH_SIZE', type=int,\n",
    "    #                     help=\"batch size to be used in 'train' mode (default: 32)\")\n",
    "    # parser.add_argument('--epochs', dest='EPOCHS', type=int,\n",
    "    #                     help=\"epochs to be used in 'train' mode (default: 10)\")\n",
    "\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 687,
     "referenced_widgets": [
      "56d99170a1464466aec45d23378ad089",
      "ae624d419a9b4249882cdb6e8110e2fe",
      "bdbb9fcdaa764504bf5fbccdd80dda5d",
      "8e5d3d6c9bdd4401a41cac7ff218da5f",
      "8f3d74f98a744e5eac3bb4374ee8e0a1",
      "3526f32499674389b033a40e2471be0e",
      "ef1108b0359c49d780c6724998d059a5",
      "10a2df5dd6f14ab38dab71ab263ebe14",
      "a5786987ba3a43e098c095f8ab8d29f9",
      "70206041b56040238b1dfc72108a7a93",
      "727097e4f6454084a77120bcc96f9a83",
      "0fc01dec8a4e46948624a7e8226db953",
      "b0ba7d0420084f5e98b32a88b901b8d0",
      "04541cbd32f44cb8942a5635acf21ad7",
      "6b8c9b19dcd944869bc05336271d7ca3",
      "4779a79dafa841e8b0633080aed65049",
      "525446b25ac849838b830cfce21936b0",
      "4ce4ff3e676240b6b0145e4c63f6a977",
      "9b458812bd2247e1a13f6b2fc0a84511",
      "7c7fafac090148c8ac1c1e152c97649b",
      "e0a44758931e4ac2a9e338a66e0dcebb",
      "157d4968e33e40f88a5185bc8c5e3fea",
      "0449050a7cc842478985ba00214f894f",
      "382209ffd48440b5a19188d5f8c8a013",
      "950c7e10cebf46158e4d55d1ae6b382e",
      "d002cb2f8907480e86f2aa27901aa416",
      "4bc395ac45b04d419dd0eec14fbdd90f",
      "c6d9372d9d9346b18406471bd8073222",
      "ddd3e23a8c924b2eb5278b2349f2e31b",
      "b1be5f0a8f9e43fc9872664d3efede57",
      "12f66bc0031d4e6c9e0037ffdc4c3246",
      "d9d5fe1cb5a4413a9ead2c4bc6cd7620",
      "8f7dbbe60ed1476cb14be6e406c569f5",
      "2436ae5443e8411ab4d9c5d5c45dc1e9",
      "d2c25d10c29c4fee91124d94e2e6966b",
      "f3d4707dbbe445658b2cf76f02eb3706",
      "c01c226982eb40c9ae2b9ea580ab6bf9",
      "93dc4338ef3e4249ba69cdbcb8b8dcf9",
      "03ec0c25cdb447df87d03d754c2c1f74",
      "9550ec7e44324d8998084b52bdd088b2",
      "7bc5e4f2ce55495c9dee777beb855272",
      "25b5066f3cef4917ab5e799efa551ffb",
      "21dbe6ac8a7c4f27a3017ede2243ef3a",
      "e28402e11a94496387929f993839036a",
      "aab1b969cef149eda67a46c4c43e6e14",
      "2b836edf91914557bc1da01365ca5032",
      "ca47b4406b2743c6af82291d84080de3",
      "b311f1e137194b0c8d77b98973cc32f9",
      "1150370a2d984a2c9bbb24fe97bf1bf7",
      "08958863d9494997a72f2f6af2fd7e26",
      "5aae76e3c2d044a789932a9807bbec4d",
      "eed64ca3060b42a3a59300f200e5dc44",
      "b7025dbb5a6c479b84a64af9feaa383f",
      "b343006d60df47cd8f26a17a5f76454a",
      "3bf40dbb2f64491bb9a5bb4fbd4d98c2",
      "816b2bb15233439ab48e14011d83f154",
      "134ee8b0d6ff4ae08258e69449adb8a8",
      "cc79e680c0d24f5ba9b69c6a4b2cbb91",
      "bd8bd64bd5e84446804c804ef39161c3",
      "7802b97b0f3a462db388d6476d10100d",
      "80efa213c7824600808c10c1d78a3b08",
      "6b7ab82e4fa2496eb0b40170a20b9d4a",
      "3467bb6b14f64a02a8bb806c5dd0e895",
      "22b64cc9d6074120ab157d2cdc7d4dc7",
      "4dedc28e106a44eaa72f67d68b3ff9a7",
      "6eb860dabf1e4a81ba905fcebbbffb96",
      "f7f826444e36492b93d2349876245251",
      "6e7a891079c84922bd95cf94988b5d46",
      "10c2235a87f74ce7b864693417150eda",
      "0598c37134514d4aa997f625f26bcb36",
      "127c9f43339b44dcad39e16247af20c8",
      "c25025bd9dbf41fb84eae1b8e919ac62",
      "1ba4003fbc3846f899ae1d49bb6b208d",
      "76d63fe2302d4169b14df06f5e44343b",
      "b24bb27bf3074ff99a3a3470c0780aa6",
      "90e3a94c1c3e453da07ce8286a55b139",
      "67d6c710024b40309a0aed404ee78783",
      "1d1bf7e2d4c74a9d9d84f1e09587c38a",
      "c7772a4f780f4862a24f19879fefef36",
      "ce4c796e2dd143e582516ed3e7e31d53",
      "11eb384560c04002a8279e87273af865",
      "a49ef9e47ee94dcb980a0625d2250875",
      "7e8b402bd6574017b9c5e7c411844182",
      "9c0534af4a1943cfb89ecd62132c809a",
      "25343ddfc775421b97f80cad73080ee8",
      "fbee6d1c9f9843b1b596255a6cc5b573",
      "481325ea55e749ada119e117181f09fe",
      "fbb78fc5b1f0460082773ac0cf22365f",
      "6f67a8059a264cf08fe77055272ac50d",
      "d7b4a85ef30f4dcabc9990cf810fc857",
      "af4ac496b379493783230b153b082fa2",
      "2bc6db95d4c045cc885c4c37d780c224",
      "50094b05f9494ee9b58287010c66f3c4",
      "d82d922ae62b46cbbdd55ddb0583be93",
      "91c4c0606e5a4ef0b960e186fcc31aac",
      "a22f1e4bb611449c927e09f481c99e52",
      "7f2ae1c56c1a4c83b8ff103c74f379e4",
      "3f4cdbc7140a4ecc9d0072bdb2db533c",
      "6beb6c4c9c754367aeae0cfc41971847",
      "74fc95fd083944b8b1435a290adba0dc"
     ]
    },
    "id": "K3U0KEKV2m4W",
    "outputId": "7c33eeec-a80e-469e-dff8-d76141b47352"
   },
   "outputs": [],
   "source": [
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xn9HBJQWPp2C",
    "outputId": "3629bd97-91ba-43ce-ab1a-f911cb501a96"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTBwS_4ofiXS"
   },
   "outputs": [],
   "source": [
    "class Pippo():\n",
    "    def __init__(self,a):\n",
    "        self.a = a\n",
    "    def __call__(self):\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKiSo40bgdo0"
   },
   "outputs": [],
   "source": [
    "asd = Pippo(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iswRWA7Ch8yY",
    "outputId": "87b8878a-49b9-4d69-c5da-af31d62b5f0a"
   },
   "outputs": [],
   "source": [
    "asd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcRgs5eih-vA",
    "outputId": "bc1bb73b-55a0-4175-fd3d-c5b2099b4191"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uX0sgJZ7lqCO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deepTarget project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
